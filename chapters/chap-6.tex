% !TEX root = thesis.tex

\startcomponent chap-7
\environment common
\product thesis

\startchapter
	[reference=chap:knowledge-uncertainty,
	 title={From Certain Values to Uncertain Values}]
  
  Requirements engineers faces uncertainties in a variety of facets of the
  system-to-be \cite[Che09c,Esf13a,Let14a,Wel10a,Per14a]. In particular, the
  extend to which the requirements are satisfied is uncertain. This type of
  uncertainty is the physical uncertainty, referring to the system phenomena
  \cite[OHa06a,Vos08a,Per14a]. For example, the chance of a sensor breaking
  down might be defined by a probability. This is domain-level form of
  uncertainty can be reduced by changing the system under
  consideration\emdash{}e.g., by introduction of appropriate countermeasures as
  seen in the previous chapter. However, the extend to which these
  probabilities are accurate is uncertain as well. This uncertainty is the
  knowledge uncertainty, referring to the assessment of physical uncertainty by
  experts \cite[OHa06a,Vos08a,Per14a]. Our imperfect knowledge of what the
  exact probability value is, might lead us to estimate it with some uncertainty
  margin. Such meta-level form of uncertainty can be reduced through further
  studies, consultation of more experts, increased experience, run-time
  monitoring of relevant events or data, and so forth. So far, few risk-based
  requirement engineering frameworks support such knowledge uncertainty; the
  techniques for assessing and controlling the risks are therefore not
  applicable.

  This chapter explains how knowledge uncertainty is captured, how the
  uncertainty margins of leaf obstacles impacts the uncertainty margins of
  high-level goals, and how such uncertainty margins can be reduced. The
  techniques proposed in the previous chapters to identify critical and likely
  risks, and to select most appropriate countermeasures, are extended to
  support uncertain estimates.

  We extend the framework with knowledge uncertainty captured by probability
  distributions. Such distributions allows experts to specify their knowledge
  about the satisfaction rates of leaf obstacles. Repeating the single-value
  propagation procedure for computing the satisfaction rate of high-level goals
  using sampled satisfaction rates for the leaf obstacles provides the
  satisfaction rates for the high-level goals with their uncertainty margins.
  Two metrics, the violation uncertainty and the uncertainty spread, summarizes
  the uncertainty about the satisfaction rates. As a results, the metrics
  provides finer-grained assessment for the critical obstacles. In presence of
  uncertainty, selecting the most appropriate countermeasures guarantee that
  the high-level goals are satisfied, up to an uncertainty threshold.
  
  The chapter is organized as follows.
  \in{Section}[sec:capturing_k_uncertainty] presents how knowledge uncertainty
  is captured through probability distributions.
  \in{Section}[sec:assessment_k_uncertainty] presents how leaf obstacles are
  assessed, how the knowledge uncertainty propagates through the obstacle and
  the goal model, and how obstacles with critical uncertainty margins are
  identified. \in{Section}[sec:selection_k_uncertainty] details how
  countermeasures are selected in presence of knowledge uncertainty.

    % \in{Section}[sec:update_k_uncertainty] shows how the knowledge uncertainty might be used at runtime to provide more refined estimates.

	\startsection
    [reference=sec:capturing_k_uncertainty,
     title={Capturing Knowledge Uncertainty}]
  
    Probability distributions are commonly used in risk analysis for
    representing the uncertainty about single-point values of random variables
    \cite[Vos08a]. A probability distribution is a function assigning a
    specific probability to each possible single-point value in some domain.
    The domain considered here is the set of possible probability values, that
    is, the interval $[0, 1]$. This section extends the probabilistic
    goal/obstacle specification language to capture the uncertainty about
    estimated satisfaction rates of behavioral goals, assumptions, and
    obstacles in the system-to-be.

		\subsection {Uncertainty about Satisfaction Rates}
    
      Let $A$ denote a probabilistic assertion specifying a goal, an assumption
      (prescriptive or descriptive), or an obstacle in the goal/obstacle models.

      \startdefinition[def:sat_uncerainty]{Satisfaction Uncertainty}
      
        The satisfaction uncertainty for assertion $A$, denoted by $su_A$, is
        defined by a probability distribution over its single-point
        probabilities of satisfaction.
      
      \stopdefinition

      For example, \in{Figure}[fig:root_goal_uncertainty] shows the probability
      distribution capturing the satisfaction uncertainty $su_G$ for $G:$
      \goal{Achieve [Make Up Water Provided When Loss Of Cooling]}.
      (\in{Section}[sec:computing_uncertainty] describes how such distribution
      can be computed.) As seen in \in{Figure}[fig:root_goal_uncertainty], the
      probability of satisfaction of this goal lies between 71.3\% and 84.1\%,
      with a more likely value around 80\%. This multi-value estimate does not
      completely meet the goal's RSR, prescribed to be 80\% as depicted by the
      red line on the right.

      \placefigure[here]
     	  [fig:root_goal_uncertainty]
     	  {Satisfaction uncertainty for \goal{Achieve [Make Up Water Provided When Loss Of Cooling]}.}
        {\externalfigure[../images/chap6/usr_make_up_water_provided.pdf]}
  
      A goal satisfaction uncertainty arises from obstacle uncertainties,
      domain hypothesis, and domain assumptions satisfaction uncertainties. For
      example, the certainty about the extent to which the goal \goal{Achieve
      [Make Up Water Provided When Loss Of Cooling]} is satisfied depends on
      our certainty about the extent to which, among others, the obstacles
      \obstacle{Pump Electrical Failure} or \obstacle{No Power Available} are
      satisfied. \in{Figure}[fig:obstacle_uncertainty] illustrates the
      satisfaction uncertainty for these obstacles.
      
      \placefigure[here]
     	  [fig:obstacle_uncertainty]
     	  {Satisfaction uncertainty for obstacles.}
        {\startcombination[2*1]
          {\externalfigure[../images/chap6/usr_no_power_available.pdf]}{\tfx(\obstacle{\tfx No Power Available})}
          {\externalfigure[../images/chap6/usr_electronic_failure.pdf]}{\tfx(\obstacle{\tfx Pump Electrical Failure})}
         \stopcombination}
	
		\subsection[sec:uncertainty_metrics]{Uncertainty Metrics for Goal Satisfaction}
	
      The probability to observe a satisfaction rate of at least $p$
      for goal $G$, denoted $SU_G (p)$, is obtained as follows:

      \startformula

      		SU_G (p) = \int_{0}^{p} su_G(x)

      \stopformula

      This probability corresponds to the area delimited by the $su_G$ curve
      and value $p$. In \in{Figure}[fig:root_goal_uncertainty], the chance of
      the goal \goal{Achieve [Make Up Water Provided When Loss Of Cooling]}
      being satisfied in at least 80\% of cases is given by:

      \startformula
      
      		SU_{\goal{Achieve [Make Up Water Provided When Loss Of Cooling]}} (0.8) = 0.7313

      \stopformula
      
      It corresponds to the red area in \in{Figure}[fig:root_goal_uncertainty].
      As shown in \in{Section}[sec:computing_uncertainty], $su_G$ can be
      computed by up-propagation from leaf obstacles to their root in obstacle
      refinement trees and then by up-propagation through the goal model. Given
      $su_G$, it is easy to compute $SU_G(p)$.

    	Two metrics may be defined to capture {\it (a)} our degree of certainty
    	that the goal's RSR will be met; and {\it (b)} the spread of satisfaction
    	uncertainty below this RSR.

      \noindent {\bf Goal violation uncertainty.} The {\it violation
      uncertainty} for a probabilistic goal $G$, denoted by $VU (G)$, is the
      proportion of satisfaction uncertainty falling below the goal's
      RSR\emdash{}that is, the probability of being below this threshold. It is
      obtained as follows:

      \startformula

      		VU (G) = SU_G (RSR(G)) = \int_{\clap{0}}^{\clap{RSR(G)}} su_G(x)

      \stopformula
      
      For discrete values, this is computed as the ratio between the number
      of values below RSR and the total number of values.

      Graphically, this violation uncertainty corresponds to the surface below
      the satisfaction uncertainty curve up to the goal required satisfaction
      rate. In our example, the violation uncertainty of \goal{Achieve [Make Up
      Water Provided When Loss Of Cooling]} is $.7313\%$. This means that it is
      73.13\% certain that the goal will not meet its RSR of 80\%. In
      \in{Figure}[fig:root_goal_uncertainty], most of the curve is on the left
      of the goal's RSR.

      \noindent {\bf Uncertainty spread.} Violation uncertainties only capture
      how much uncertainty lies below the required satisfaction rates. The same
      surface might however take many shapes with more or less spread. Such
      spread may help making decisions. It might be less problematic to have
      the uncertainty closer to the RSR than equally spread down to zero. If
      the uncertainty is close to the goal's RSR, a small change to this RSR
      might drastically change the violation uncertainty score.

    	The {\it uncertainty spread} for a goal $G$, denoted by $US (G)$,
    	measures the spread of uncertainty below the goal's RSR. It is defined as
    	the semi-standard deviation of its probability distribution with respect
    	to this RSR. The semi-standard deviation of the distribution is used here
    	for measuring spread; It differs from standard deviation as only values
    	below a threshold are taken into account \cite[Roh11a]\emdash{}here, below
    	the RSR.

      \startformula
      
        US(G)^2 = \int_{\clap{0}}^{\clap{RSR(G)}} \left(x - RSR(G)\right)^2 \cdot su_G(x)
      
      \stopformula

      For discrete values, this quantity can be computed as follows:
      
      \startformula
      
        US(G) = \sqrt{\frac{1}{k}\sum_{x_i} \left(x_i - RSR(G)\right)^2}
      
      \stopformula

      where $x_i$ are the discrete values of G's satisfaction uncertainty that
      fall below $RSR(G)$, and $k$ denotes the number of such values.

    	Uncertainty spread values need to be interpreted according to the shape
    	of the curve. Whatever the shape, however, Chebyshev's inequality states
    	that at least 75\% of the data are at most at 2 spread values from $RSR(G)$
    	\cite[Wac07a]. If the goal's satisfaction uncertainty fits a specific
    	probability distribution, tighter bounds can be obtained.

    	Back to our example, the goal \goal{Achieve [Make Up Water Provided When
    	Loss Of Cooling]} has a mean of $78.63\%$ and an uncertainty spread of
    	$0.0269$. According to Chebyshev's inequality, this means that we have at
    	least 75\% of the satisfaction uncertainty between $(78.63 - 2\times 2.69)
    	= 73.25\%$ and $(78.63 + 2\times 2.69) = 84.01\%$.
  
    \subsection{Eliciting Distributions for Leaf Obstacle Satisfaction}
 
      To facilitate the elicitation of distribution functions for leaf
      obstacles from domain experts, discrete points may be used that can be
      fitted to specific probability distributions\emdash{}such as Beta,
      PERT, Triangular, etc. \cite[Vos08a]. A quantile is a single
      probability value attached to a cumulative probability. It indicates
      the cumulative probability to observe a single value of probability of
      satisfaction \cite[Bed01a]. The 50\high{th} quantile, called mode, is the most
      likely probability of satisfaction.
 
   	  To further support such elicitation from domain experts, databases
   	  providing estimates for quantiles can be used; they are available in a
   	  wide range of domains\emdash{}e.g., aerospace, health, bank, nuclear,
   	  chemical, gas, water pollution, and so forth \cite[Coo08a]. For example,
   	  our estimates were based on reliability databases and published
   	  historical data in various industries \cite[Ayy14a,Mil92a,Cen92a].
   	  Reliable techniques are also available for obtaining accurate single
   	  values or distribution estimates from trained experts \cite[OHa06a].
 
   	  For the leaf obstacle \obstacle{Pump Mechanical Failure}, as seen in
   	  \in{Figure}[fig:obstacle_fragment_chap7], an expert might estimate that
   	  there are at least 10\% chances of observing at least 10 mechanical
   	  failures of a pump out of 100 days; at least 50\% chances of observing at
   	  least 15 such occurrences; and at least 90\% chances to observe at least
   	  30 of them. The 10\high{th}, 50\high{th} and 90\high{th} quantiles are
   	  $.1$, $.15$, and $.3$ for the probability of satisfaction of this leaf
   	  obstacle, respectively. \in{Figure}[fig:uso_pump_failure] shows the
   	  satisfaction uncertainty for this obstacle.
      
      \placefigure[top]
     	  [fig:obstacle_fragment_chap7]
     	  {An obstacle tree fragment for \goal{Achieve [Make Up Pump Motor On When Water Requested]}.}
        {\externalfigure[../images/chap6/obstacle_fragment_chap7.pdf]}
  
      \placefigure[here]
     	  [fig:uso_pump_failure]
     	  {Satisfaction uncertainty for \obstacle{Pump Mechanical Failure}.}
        {\externalfigure[../images/chap6/uso_pump_failure.pdf]}
      
      In \in{Figure}[fig:uso_pump_failure], the 10\% of probabilities before
      the 10\high{th} quantile were distributed uniformely down to $0$; The
      10\% after the 90\high{th} quantiles uniformely up to $1$. So, the
      0\high{th} quantile is $0$ and 100\high{th} quantile is $1$. This spread
      is controlled by the {\it overshoot factor}; it determines the 0\high{th}
      and 100\high{th} quantiles. \in{Figure}[fig:uso_pump_failure_of5] shows
      the same estimates with a $.25$ overshoot factor. In that example, the
      specified values are spread between 10\% and 30\%, so the spread is 20\%.
      The overshoot is $.25 \times .20 = .05$. So the 0\high{th} quantile is
      $.1 - .05 = .05$ and 100\high{th} quantile is $.30 + .05 = .35$. As we
      can see by comparing \in{Figure}[fig:uso_pump_failure] and
      \in{Figure}[fig:uso_pump_failure_of5], this might have a important impact
      on the probability distribution.
  
      \placefigure[here]
     	  [fig:uso_pump_failure_of5]
     	  {Satisfaction uncertainty for \obstacle{Pump Mechanical Failure} with a $.25$ overshoot factor.}
        {\externalfigure[../images/chap6/uso_pump_failure_of5.pdf]}
  
  
  \stopsection
	
	\startsection
    [reference=sec:assessment_k_uncertainty,
     title={Obstacle Assessment with Knowledge Uncertainty}]
	
    As seen in \in{Chapter}[chap:assessing], identifying critical and likely
    obstacles is important for the next resolution step. This section shows how
    identification of critical and likely obstacle is extended to support
    knowledge uncertainty. The satisfaction rate of some obstacles might be
    highly uncertain; theses obstacles shall have their uncertainty margins
    reduced. Our general approach for achieving this consists of the following
    steps:
    
    \startitemize
    
      \item The accuracy of estimates for the satisfaction rate of leaf
      obstacles is increased by combining those elicited from multiple experts
      and/or data sources (\in{Section}[sec:eliciting_more_accurate_estimates]);
    
      \item Goal satisfaction rates and their uncertainty margin are computed
      from those more accurate estimates
      (\in{Section}[sec:computing_uncertainty]);
    
      \item Leaf obstacles are prioritized according to their impact on the
      satisfaction of top-level goals using the metrics previously introduced
      (\in{Section}[sec:finding_critical_obstacles_uncertainty]).
    
    \stopitemize
  
		\subsection
      [title={Eliciting More Accurate Estimates},
       reference=sec:eliciting_more_accurate_estimates]
         
      The first step of our approach consist of eliciting estimated
      satisfaction rates for the leaf obstacles in the obstacle AND/OR
      refinement trees built during the obstacle identification phase. To
      reduce uncertainty margins of the high-level goals, such estimates should
      be as adequate and accurate as possible.
  
      As discussed in \in{Chapter}[chap:assessing], the use of multiple sources
      or multiple experts is generally recognized to increase the accuracy of
      estimates and more mathematical approaches are recognized to produce
      accurate results \cite[Coo91a, Cle99a]. Among available approaches, some
      mathematical approaches are aimed at characterizing expert judgements by
      comparative assessment between their estimates and known quantities. The
      derived characteristics are then used for combining estimates from
      multiple experts.
      
      \noindent In the proposed probabilistic framework, obstacles may be
      annotated with estimated quantiles from multiple experts, e.g.,
      
      \startkaosspec
        \ObstacleName { Pump Mechanical Failure}
        \KaosAttribute {Definition} {The pump is not pumping water due to a mechanical failure.}
        \KaosAttribute {Probability} {[Expert1] quantiles ($10\%$,$15\%$,$30\%$)}
        \KaosAttribute {Probability} {[Expert2] quantiles ($20\%$,$25\%$,$35\%$)}
      \stopkaosspec
    
      However, experts are not all equal. Some might systematically over- or
      under-estimate probabilities of satisfaction; provide very large or very
      narrow estimates; provide estimate ranges that are accurate or not; and
      so forth. To get more accurate estimates, we may compare those provided
      by multiple experts with known values in order to characterize each
      expert. This is known as calibration \cite[Bed01a].
    
      A {\it calibration variable} is a quantity whose exact single-point value
      is known. In our context, a calibration variable should be closely
      related to a leaf obstacle\emdash{}typically, a leaf obstacle whose satisfaction
      rate is known, an agent/resource failure whose failure rate is known from
      reliability databases \cite[Akh01], etc. In our running example, three
      calibration variables might be identified: \obstacle {Push Button
      Broken}, \obstacle {Temperature Sensor Broken} and \obstacle {Fire
      Sprinkler Broken}. \in{Table}[tab:estimate_calibration] summarizes
      estimates and known values for two of them. The X\% columns show the
      quantiles estimated by the corresponding expert for the calibration
      variable.
    
      \placetable[top][tab:estimate_calibration]
        {Expert estimates for calibration variables.}
        {\setupTABLE[c][each][align={middle,lohi},frame=off,offset=0pt]
        \setupTABLE[c][1,2][align={right,lo}]
        \setupTABLE[r][1][style=bold,bottomframe=on,boffset=4pt]
        \setupTABLE[1,2][1][align={right,lohi}]
        \setupTABLE[r][2][toffset=4pt]
        \setupTABLE[c][3,4,5,6][align={middle,lohi}]
        \setupTABLE[c][6][align={middle,lo}]
        \setupTABLE[c][3,4,5][width=1.5cm]
        \setupTABLE[c][1][width=5cm]
        \setupTABLE[c][2][width=1.8cm]
        \setupTABLE[c][6][width=1.8cm]
%        \setupTABLE[c][3][leftframe=on]
%        \setupTABLE[c][5][rightframe=on]

      \switchtobodyfont[small]
        \bTABLE

          \bTR \bTD Variable \eTD \bTD Expert \eTD \bTD $10\%$ \eTD \bTD $50\%$ \eTD \bTD $90\%$ \eTD \bTD Known Value \eTD \eTR

          \bTR \bTD[nr=2] Push Button Broken  \eTD       \bTD Expert 1   \eTD \bTD   $3.81\%$   \eTD \bTD   $4.28\%$   \eTD \bTD   $4.76\%$   \eTD \bTD[nr=2]   $4.37\%$   \eTD \eTR
          \bTR [boffset=4pt]                             \bTD Expert 2   \eTD \bTD   $3.96\%$   \eTD \bTD   $4.45\%$   \eTD \bTD   $4.94\%$   \eTD \eTR
          \bTR \bTD[nr=2] Temperature Sensor Broken \eTD \bTD Expert 1   \eTD \bTD   $2.31\%$   \eTD \bTD   $2.59\%$   \eTD \bTD   $2.88\%$   \eTD \bTD[nr=2]   $4.37\%$   \eTD \eTR
          \bTR [boffset=4pt]                             \bTD Expert 2   \eTD \bTD   $4.18\%$   \eTD \bTD   $4.70\%$   \eTD \bTD   $5.22\%$   \eTD \eTR
          \bTR \bTD[nr=2] Fire Sprinkler Broken \eTD     \bTD Expert 1   \eTD \bTD   $0.01\%$   \eTD \bTD   $0.02\%$   \eTD \bTD   $0.03\%$   \eTD \bTD[nr=2]   $0.02\%$   \eTD \eTR
          \bTR                                           \bTD Expert 2   \eTD \bTD   $0.02\%$   \eTD \bTD   $0.022\%$  \eTD \bTD   $0.024\%$  \eTD \eTR

        \eTABLE}
      
      Two techniques are available for characterizing multiple experts and
      combining their estimated quantiles. We instantiate them to our context
      in order to combine multiple quantiles on a leaf obstacle into a single
      satisfaction uncertainty.
      
      \noindent {\it Cooke's technique} characterizes each expert through a single weight
      \cite[Coo91a]. This weight combines a calibration score and an
      information score. The {\it calibration score} measures how close the
      expert's estimate is to the known value of the calibration variable; the
      closer the probability estimated for this value, the higher the score.
      The {\it information score} measures how precise the estimates are; the
      narrower the estimates, the greater the score. The satisfaction
      uncertainty combining the quantiles from multiple experts is obtained as
      a weighted sum of these quantiles. In our example, {\it Expert 1} gets an
      information score of $2.45$, a calibration score of $0.76$, and a weight
      of $0.59$; {\it Expert 2} gets an information score of $2.75$, a
      calibration score of $1$, and a weight of $0.40$.
      \in{Figures}{(a)}[fig:obstacle_combination] and
      \in{}{(b)}[fig:obstacle_combination] show the quantile function of both
      experts; \in{Figure}{(c)}[fig:obstacle_combination] shows the resulting
      satisfaction uncertainty.
      
      \noindent {\it Mendel-Sheridan's technique} combines the quantiles of the experts
      by use of a Bayesian calibrator/estimator \cite[Men89a]. It first
      computes a minimally informative distribution for the expert's
      characteristics. This a priori distribution considers each expert to be
      unbiased; it does not weight any probability more than others. This a
      priori distribution is then updated with respect to the expert's
      quantiles and the known value of calibration variables. The resulting
      distribution is used to combine the quantiles of the experts into a
      satisfaction uncertainty for our leaf obstacle.
      \in{Figure}{(d)}[fig:obstacle_combination] shows the resulting
      satisfaction uncertainty using this technique.
      
      Which technique performs best remains an open question; it may depend on
      the application domain, the experts, and the number of calibration
      variables \cite[Coo91a].
  
      \placefigure[top]
     	  [fig:obstacle_combination]
     	  {Satisfaction of \obstacle{Pump Mechanical Failure}.}
        {\startcombination[2*2]
          {\externalfigure[../images/chap6/quantiles_pump_failure_e1.pdf]}{\tfx(a) Expert 1’s estimate}
          {\externalfigure[../images/chap6/quantiles_pump_failure_e2.pdf]}{\tfx(b) Expert 2’s estimate}
          {\externalfigure[../images/chap6/quantiles_pump_failure_cook.pdf]}{\tfx(c) Combined estimates using Cooke’s technique}
          {\externalfigure[../images/chap6/quantiles_pump_failure_ms.pdf]}{\tfx(d) Combined estimates using Mendel-Sheridan’s technique}
        \stopcombination}
	
		\subsection
      [title={Computing Satisfaction Rates and Their Uncertainty},
       reference=sec:computing_uncertainty]
	
      The procedures presented in \in{Chapter}[chap:assessing] computes
      single-point probability values for goal satisfaction from single-point
      probability values for satisfaction of the obstructing leaf obstacles. To
      build a complete probability distribution for a top goal in the goal
      model, we need to sample the satisfaction uncertainty for these leaf
      obstacles. To sample a satisfaction uncertainty, a random number between
      $0$ and $1$ is uniformly picked. The inverse of the cumulative
      distribution function associated with the satisfaction uncertainty is
      then used to get a corresponding probability of satisfaction. More likely
      probabilities of satisfaction are thereby picked more often than less
      likely ones.
      
      The probability of satisfaction of a top goal is then computed for a
      given sample. The sampling is repeated a large number of times to obtain
      a set of probabilities of satisfaction for this top goal. The obtained
      set of probabilities can then be aggregated into a distribution by using
      their frequency (This is known as {\it Kernel Density Estimation}
      \cite[Fri01a].) For example, by repetitive sampling for the leaf
      obstacles, the following probabilities of satisfaction for the top goal
      \goal{Achieve [Make Up Water Provided When Loss Of Cooling]} might be
      obtained:
      
      \startformula
        0.762, 0.690, 0.686, 0.675, 0.736, 0.908, 0.794, 0.555, 0.678 ...
      \stopformula
      
      Using kernel density estimation techniques, we can build the distribution
      shown in \in{Figure}[fig:goal_combination].
      \in{Figure}{(a)}[fig:goal_combination] shows, in black, the satisfaction
      rate when combining expert estimates using Cook's technique.
      \in{Figure}{(b)}[fig:goal_combination] shows, in black, the satisfaction
      rate using the Mendel-Sheridan's technique. The two curves in gray
      corresponds to the satisfaction rates obtained when using only one of the
      two expert.
      
      \placefigure[here]
     	  [fig:goal_combination]
     	  {Satisfaction of \goal{Achieve [Make Up Pump Motor On When Water Requested]}.}
        {\startcombination[2*1]
          {\externalfigure[../images/chap6/quantiles_make_up_water_provided_cook.pdf]}{\tfx(a) Combined estimates using Cooke’s technique}
          {\externalfigure[../images/chap6/quantiles_make_up_water_provided_ms.pdf]}{\tfx(b) Combined estimates using Mendel-Sheridan’s technique}
        \stopcombination}
  
      The number of sampling required depends on the satisfaction uncertainties
      for the leaf obstacles, the numerical precision to achieve, the time
      available to solve the problem, and so forth.
	
		\subsection[sec:finding_critical_obstacles_uncertainty]{Finding Critical Obstacles}

      The third step in the overall approach consists of prioritizing obstacles
      according to the metrics introduced in
      \in{Section}[sec:uncertainty_metrics], that is, the resulting violation
      uncertainty and uncertainty spread for top-level goals.
      
      The impact of each single leaf obstacle on the goal model must be
      assessed individually. For a given top goal, the procedure in the
      previous section is applied for each leaf obstacle with the satisfaction
      probability of all other leaf goals being set to $0$. The violation
      uncertainty and the uncertainty spread for the considered top goal are
      then computed according to their definition in
      \in{Section}[sec:uncertainty_metrics]. The result may be represented on a
      scatter plot, called violation diagram, to highlight the most critical
      obstacles to this top goal. When all likely and critical individual
      obstacles are resolved, the violation uncertainty and uncertainty spread
      may be computed again with pairs of leaf obstacles in order to build a
      new violation diagram; then with triples, and so forth.
      
      Our running example contains 12 leaf obstacles. The violation uncertainty
      and uncertainty spread for the top goal \goal{Achieve [Make Up Water
      Provided When Loss Of Cooling]} are $90.70\%$ and $0.1818$, respectively.
      \in{Figure}[fig:violation_diagram] shows the corresponding violation
      diagram; the $x$-axis is logarithmic. The following observations can be
      made from it.
      
      \placefigure[top]
     	  [fig:violation_diagram]
     	  {Violation Diagram.}
        {\externalfigure[../images/chap6/violation_diagram.pdf]}
      
      \startitemize
      
        \item The obstacle \obstacle{Pump Mechanical Failure} is clearly a critical one.
        The violation of the top goal it causes is mostly certain (with $78.3\%$).
        
        \item The top goal violation caused by \obstacle{Pump Electrical Failure}
        is uncertain, however the uncertainty spread is very
        low. This indicates that the uncertainty is probably close to the
        goal's RSR. This obstacle might thus not be that critical as a slight
        change in the goal's RSR might drastically change the violation
        uncertainty.
      
        \item The top goal violation caused by \obstacle{Power Supply Failure}
        is also uncertain but its uncertainty spread is high. The uncertainty
        is probably not close by the RSR.
      
        \item The leaf obstacle \obstacle{Power Cabling Failure} is not causing
        the highest violation uncertainty nor uncertainty spread.
        
        \item For the remaining 8 obstacles, it is certain that they do not
        individually prevent the root goal from reaching its RSR.
      
      \stopitemize
      
      As \in{Figure}[fig:violation_diagram2] shows, the most critical pairs of
      obstacles contains a critical obstacle identified in the violation
      diagram containing only single obstacles. This suggests proceeding
      iteratively on the size of obstacle combinations.
      
      \placefigure[here]
     	  [fig:violation_diagram2]
     	  {Violation Diagram.}
        {\externalfigure[../images/chap6/violation_diagram_2.pdf]}
      
  \stopsection
  
	\startsection[reference=sec:selection_k_uncertainty,title={Selecting Countermeasures with Knowledge Uncertainty}]
  
    Previous section showed how critical and likely obstacles are highlighted.
    When countermeasures to these are identified, the most appropriate ones
    shall then be selected. Selecting most appropriate countermeasures in
    presence of knowledge uncertainty requires a relaxation of safe selection.
    In presence of uncertainty, a safe selection ensure that the violation
    uncertainty is bounded.
    
    \startdefinition{Safe Selection with Uncertainty}
    
      Given a violation uncertainty threshold $t$, a {\it safe selection} of
      countermeasures is a set of countermeasures that, once integrated,
      guarantees that the violation uncertainty of the high-level goals is
      lower than $t$.
    
    \stopdefinition
    
    Not all safe selections are equal; the most appropriate selections minimize
    the violation uncertainty\emdash{}i.e. maximize the fraction of uncertainty
    that is above the required satisfaction rate\emdash{}, while minimizing the
    uncertainty spread\emdash{}i.e. the closer the uncertainty to the RSR, the
    better. According this relaxed definition, selecting the most appropriate
    countermeasures amount to solving the following optimization problem:
    
    \startitemize

      \item Finding the minimal cost for guaranteeing the violation uncertainty
      threshold of the high-level goals,

      \item Finding the selections that minimize the violation uncertainty and
      the uncertainty spread of the high-level goals given this cost.

    \stopitemize
  
    Back to our running example, the minimal cost to guarantee a violation
    uncertainty below $5\%$ ranges from $1$ to $6$. There are 63 possibles
    selections, among them, 48 are safe. The countermeasure selection that
    minimize the violation uncertainty only contains \goal{Achieve [Cooling
    System Repaired]}. The resulting violation uncertainty and uncertainty
    spread is $2.9\%$ and $0.3773$ respectively; versus $89.8\%$ and $.1719$
    respectively, without the countermeasure goal.
  
  \stopsection
  
  \startsection[title={Summary}]
  
    The quantitative technique presented in this chapter allows analysts to
    cope with knowledge uncertainty about satisfaction rates of system goals
    and their obstructing obstacles. The formal probabilistic framework
    presented in the previous chapters is extended to explicitly reason about
    uncertain estimates. The impact of estimation uncertainty is measured on
    top-level goals through two metrics: goal violation uncertainty and
    uncertainty spread. Satisfaction rates and their uncertainties for top
    goals are computed by up-propagation through obstacle and goal refinement
    trees, from leaf obstacles whose satisfaction rate and uncertainty need be
    estimated. As a result, more critical leaf obstacles may be highlighted for
    resolution and most appropriate countermeasures can be selected. To reduce
    the uncertainty margins, our approach allows estimates from multiple
    sources to be combined.
    
    Beyond frameworks for goal-oriented RE, other RE and risk analysis
    frameworks might benefit from our techniques. In particular, FTA might
    integrate similar metrics for problematic uncertainties together with means
    for reasoning about risk consequences and combinations of multiple expert
    estimates. Other software engineering areas are faced with the problem of
    handling uncertainties about estimated quantities \cite[Fen00a]. The
    application of similar techniques appears worth considering in other
    contexts beyond RE as well.
    
    The next chapter show how the assessment and control of critical obstacles
    can be performed at run-time to provide obstacle-driven runtime adaptation.
  
  \stopsection
  
\stopchapter

\stopcomponent
